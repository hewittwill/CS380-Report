\subsection{Deep Learning for Echocardiography}

Previous bodies of work \cite{zhangFullyAutomatedEchocardiogram2018}
\cite{leclercDeepLearningSegmentation2019} \cite{hewitt2019artificial} have
proven that a deep learning based approach approaches human-level accuracy when
measuring standard clinical indices such as LVEF. \newline

Zhang et al. developed the most complete approach to the overall and is the
focus for this section of our review. In their work they developed a multistage
pipeline for the complete analysis of an echocardiogram, split into three
distinct stages - view classification, segmentation and applications. \newline

The first stage of the pipeline (view classification) classifies each cine in
the echocardiography study into 1 of 22 different views. Using a common CNN
architecture known as VGG-16, 10 random, downsized (224 x 224), grayscaled, and
normalied (pixel values ranging from 0-1) frames are extracted from each cine
and fed into the neural network, which returns a 22x1 probability
vector with each entry referring to the probability of the cine being a given
view. During validation this model achieved a 99\% accuracy for classifying each
view.
\newline

The second stage of the pipeline (segmentation) segments cines that are of a
view of interest. Zhang et al. consider Apical 2 Chamber (A2C), Apical 4 Chamber
(A4C), Parasternal Short Axis (PSAX) and Parasternal Long Axis (PLAX) views of
interest. What view is of interest is typically driven by the clinical question,
in this case being assessing LV function. Having identified the views that are
of interet in the view classification stage, the segmentation stage uses four
seperately trained encoder-decoder neural networks, with the same common
encoder-decoder architecture known as U-Net. Using the same downsampled,
normalised images as the view classification step the segmentation models take
in a frame, and return a classification mask classifying each pixel into one of
several classes. For the key views of interest for this research, the A2C and
A4C segmentation models were trained on 200 and 177 manually annotated images
respectively, generating a classification mask with four and six classes
respectively. The A2C model classifies each pixel into either background, left atrial
blood pool, left ventricular myocardium or left ventricular blood pool. The
A4C model classifying into the same classes as the A2C with the addition of the
right ventricular blood pool and the right atrium. The models converged to
Intersection over Union (IoU, a common overlap metric for image segmentation
tasks) values between 0.72 - 0.91 for structures of interest. \newline

The third stage of the pipeline (applications) offers several modular
applications, each producing some sort of analysis of cardiac structure and
function. The key application of interest for this project measures LV volume
(at both systole and diastole) and LVEF. With the newly segmented A2C and A4C
cines (of which there are generally multiple in each study) and the area length
method (whichs models the LV as a cone, to provide an approximation of volume)
the application produces an estimate of LV volume at each frame of the cine, for
each cine. A peak finding algorithm using a sliding window approach marks marks
the ES and ED frames of each cine by simply identifying minima and maxima. With
the now computed left ventricular end systolic and end diastolic volumes
(LVESV/LVEDV) the LVEF can be calculated using the standard formula: \newline

\begin{equation}
        LVEF = \frac{LVEDV - LVESV}{LVEDV} \times 100
\end{equation} \newline

The LV volume and LVEF application reports the average LVEF for each study,
across multiple views. The Median Absolute Deviation (MAD) for LVEF was
5.3\% over 3101 studies. \newline

Zhang et al. takes an interesting approach, which leans on the strengths of
automated measurement techniques. In typical clinical practice, a reading
clinician would only take 1 or 2 measurements of LVEF - but would take in to
account factors such as image quality and if there was a well defined myocardium
- blood pool boundary before taking a measurement. Additionally in modern
practice clinicians will conventionally use a more sophisticated technique to
calculate the LV volume at a given point in time - commonly the Simpsons Biplane
method - which simultaneously uses information from A2C and A4C views of the
heart and constructs a substantially more accurate geometric model of the heart.
Accuracy improving techniques as described are readily employed as a manual
approach, but are non-trivial when developing an automated approach. \newline

The complimentary approach taken by Zhang et al. addresses it as they segment
every frame in the sequence and compute less accurate volumes along every single
frame gives a sampling advantage as the automated approach can average over a
significantly greater number of samples than the manual approach. The reasoning
behind this being that having a high number of lower quality samples produces a
superior result than one or two high quality samples. \newline

In this approach however, and in most approaches found in literature, the
analysis step (where interesting information is produced) is entirely reliant on
accurate segmentations and view classifications in earlier steps. More
sophisticated approaches have been developed, such as those using 3D CNNs
\cite{ouyangEchoNetDynamicLargeNew} however 3D CNN approaches have substantially
more free variables (parameters) to train, and require significantly larger
datasets in a field where datasets are already constrained. \newline

A summary for work in the space of deep learning for echocardiography can be
simply summarised as requiring improved segmentation models. \newline

\subsection{Conditional Generative Adversarial Networks}

After their introduction by Goodfellow et al. in 2014, using GANs people have
made substantial advancements in the deep learning. The focus of this literature
review is not on GANs broadly, but on the utility of the subfield of Conditional
GANs used to synthesise echocardiography images. \newline

The study by Abdi et al. is the only substantial body of work in this space,
where they demonstrated that a GAN can be trained to produce photorealistic
images from an input condition (the segmentation mask). Their generative model
was a U-Net inspired encoder-decoder network - without the typical skip
connections. It is not immediately apparent why the authors opted to avoid skip
connections, potentially as latent representations of the segmentation masks
don't contain particularly useful information for the decoder stage of the
network across the bulk of the image (i.e. as the bulk of the photorealistic
image is noise, but the bulk of the condition is background). \newline

In the study they used a patch-based discriminator model, which assesses
the "realness" of each patch of the generated image rather than the entire
image. The patch-based discriminator was a simple CNN with 5 convolutional
layers. \newline

The most interesting innovation of this work was the use of a Least Squared
Error (LSE) which has been shown to push the probability distribution modelled
by the generator model closer to the real data distribution. The Generator and
Discriminator were co-optimized for this loss, aiming to minimize and maximize
the loss function respectively.\newline

\begin{equation}
        \mathcal{L}_{cGAN} = \mathbb{E}_{x,y}[(1-D(y,x)^{2})] + \mathbb{E}_{x}[D(y,G(y))^2]
\end{equation}

$y$ and $x$ refer to the input condition and the photorealistic image
respectively. $D(y,x)$ refers to the patch-based discriminator and $G(y)$
refers to the encoder-decoder generator model. \newline

The study by Abdi et al. demonstrated that motivating results can be generated by using
simple but powerful generator and discriminator models, all of which being well
understood in literature. The key limitation of their study was their principal
focus on ED frames, meaning the heart is at its largest size (during the
filling, pre-contraction) of the cardiac cycle. This (deliberately) limited the
distribution modelled by the GAN to that of just ED frames. This does make for a
potentially more physiologically interesting model however, as it is a
reasonable assumption that the size of the LV varies linearly at ED and ES
between patients. Whereas intermediary frames (or the change over time between
intermediary frames) is a more complex phenomena. This forseeably opens up
interesting further analysis. \newline

Other limitations include simply performing qualitative analysis of the output
of the generative model. Although interesting for this case, pixelwise
difference metrics (which was a term of their loss function) would potentially
have been an interesting analysis between the ground truth and generated test
images. There was also no analysis on the generator models sensitivity to
changes of the boundary of the segmentations. \newline

Limitations aside, the study by Abdi et al. has layed the groundwork for both this body of
work and future works in the space. \newline

\subsection{GAN-enhanced Image Segmentation}

Although the focus of this work is on the utility of GANs as a type of data
augmentation for the training of encoder-decoder neural networks, given the
primary endpoint of enhancing segmentation accuracy it would be fair to include
other segmentation techniques utilising GANs. \newline

The study by Xue et al. \cite{zhangSegGANSemanticSegmentation2018} developed a
novel approach to the GAN architecture called SeGAN where they replaced the
generator (G) model with a segmentor (S) model, and the discriminator (D) model with a
critic (C) model. They also introduced a novel multiscale feature loss function,
which minimizes S and maximizes C during training.
\newline

The principle innovation in this work is the proposed multiscale objective loss
function, which measures the difference between the generated segmentation and
the ground
truth segmentation at multiple layers within the critic. The advantage of this
is forcing the critic to learn higher-order features that capture actual spatial
relationships between the pixels, rather than learning a trivial solution which
a conventional GAN would - the binary "real" or "fake" classification. \newline

This brief review of a pivotal body of work is to give wider context to the
space of medical imaging GANs. There aren't immediate justifications of
superiority of the GAN approach, although superior segmentation accuracy
measurements were recorded on several datasets. One interesting qualitative
result of the study was the comment that their segmentations appeared smoother,
potentially of interest in some medical imaging problems. \newline

\subsection{Summary}

In summary, prior work in the space has convincingly demonstrated that a deep
learning approach can successfully automate the measurement of clinical indices
of an echocardiography study. \newline

What is apparent that in the bulk of the approaches is the segmentation accuracy
is the rate limiting factor. Superior segmentations would enable more accurate
and precise measurement of clinical indices. \newline